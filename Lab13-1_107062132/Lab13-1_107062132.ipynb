{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cc02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b7390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccb1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"./IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3454aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea138c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc22162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf529b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 40000\n",
      "# test data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd333145",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610afb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([200, 100]), TensorShape([200]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 200\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c59093c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_sizeï¼Œmax_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141c13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (200, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (200, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f46cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.V = tf.keras.layers.Dense(100)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
    "    \n",
    "        score = tf.transpose(tf.matmul(query_with_time_axis,self.V(values_transposed)) , perm=[0, 2, 1])\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1988fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08e676fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "# print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8561ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3c25cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5d7bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e624c73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6975\n",
      "Epoch 1 Batch 100 Loss 0.0544\n",
      "Epoch 1 Loss 0.1127\n",
      "Time taken for 1 epoch 22.545385360717773 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0308\n",
      "Epoch 2 Batch 100 Loss 0.0325\n",
      "Epoch 2 Loss 0.0299\n",
      "Time taken for 1 epoch 22.85345697402954 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0111\n",
      "Epoch 3 Batch 100 Loss 0.0027\n",
      "Epoch 3 Loss 0.0247\n",
      "Time taken for 1 epoch 22.6036958694458 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0310\n",
      "Epoch 4 Batch 100 Loss 0.0151\n",
      "Epoch 4 Loss 0.0199\n",
      "Time taken for 1 epoch 22.895638942718506 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0054\n",
      "Epoch 5 Batch 100 Loss 0.0343\n",
      "Epoch 5 Loss 0.0173\n",
      "Time taken for 1 epoch 22.639347076416016 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0314\n",
      "Epoch 6 Batch 100 Loss 0.0123\n",
      "Epoch 6 Loss 0.0155\n",
      "Time taken for 1 epoch 22.923067331314087 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0099\n",
      "Epoch 7 Batch 100 Loss 0.0047\n",
      "Epoch 7 Loss 0.0128\n",
      "Time taken for 1 epoch 22.66179633140564 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0172\n",
      "Epoch 8 Batch 100 Loss 0.0485\n",
      "Epoch 8 Loss 0.0135\n",
      "Time taken for 1 epoch 22.943843603134155 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0032\n",
      "Epoch 9 Batch 100 Loss 0.0180\n",
      "Epoch 9 Loss 0.0100\n",
      "Time taken for 1 epoch 22.686041116714478 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0029\n",
      "Epoch 10 Batch 100 Loss 0.0034\n",
      "Epoch 10 Loss 0.0087\n",
      "Time taken for 1 epoch 22.98820185661316 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "#     saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77e56580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/sentiment-analysis/ckpt-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5f3c7fbf50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a68cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c7bb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions[np.where(predictions < 0.5)] = 0\n",
    "    predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e10d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, attention_weights = evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb2352cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8429\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cafd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 1\n",
      "y_predict: 0\n",
      "changed it was terrible main event just like every match is in is terrible other \u001b[31mmatches\u001b[0m on \u001b[31mthe\u001b[0m card were razor ramon vs \u001b[31mted\u001b[0m brothers vs bodies \u001b[31mshawn\u001b[0m \u001b[31mmichaels\u001b[0m vs this was the \u001b[31mevent\u001b[0m where shawn named \u001b[31mhis\u001b[0m big monster of body guard vs kid hart first takes on then takes on jerry and stuff with the and was always very interesting then destroyed marty undertaker took on giant in another terrible match the smoking and took on bam bam and the and the world title against lex this match \u001b[31mwas\u001b[0m boring and it \u001b[31mhas\u001b[0m terrible ending however it \u001b[31mdeserves\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "of \u001b[31msubject\u001b[0m matter as are \u001b[31mand\u001b[0m broken \u001b[31min\u001b[0m many ways on many many issues happened \u001b[31mto\u001b[0m see \u001b[31mthe\u001b[0m pilot premiere in passing and just had to keep in after that to see \u001b[31mif\u001b[0m would ever get the \u001b[31mgirl\u001b[0m after seeing them all on television was delighted \u001b[31mto\u001b[0m see them \u001b[31mavailable\u001b[0m \u001b[31mon\u001b[0m dvd have to admit that it was the only thing that kept me sane whilst had to do hour night shift and developed insomnia farscape was the only thing to get me through those extremely long nights do yourself favour watch the pilot and see what mean farscape comet \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "destruction the first really bad thing is the guy steven seagal would have been beaten to pulp by seagal driving but that probably would have ended the whole premise for the movie it seems like they decided to make all kinds of changes in the movie plot \u001b[31mso\u001b[0m just plan to enjoy the \u001b[31maction\u001b[0m and do not expect coherent plot turn any sense of logic you may have it will \u001b[31myour\u001b[0m \u001b[31mchance\u001b[0m of getting headache does give \u001b[31mme\u001b[0m some hope \u001b[31mthat\u001b[0m steven seagal is \u001b[31mtrying\u001b[0m to move back towards the type of characters he \u001b[31mportrayed\u001b[0m in his \u001b[31mmore\u001b[0m popular \u001b[31mmovies\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "jane austen would \u001b[31mdefinitely\u001b[0m of \u001b[31mthis\u001b[0m one paltrow \u001b[31mdoes\u001b[0m an awesome job capturing the attitude of emma she is funny without being silly \u001b[31myet\u001b[0m elegant \u001b[31mshe\u001b[0m \u001b[31mputs\u001b[0m on very convincing british accent \u001b[31mnot\u001b[0m being british myself maybe m not the best judge but \u001b[31mshe\u001b[0m fooled me she was also excellent in doors sometimes forget she american \u001b[31malso\u001b[0m brilliant are jeremy northam and sophie thompson and law emma thompson sister and mother \u001b[31mas\u001b[0m the bates women they nearly steal the show and ms law doesn even have any lines highly recommended \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "\u001b[31mreaches\u001b[0m the point where they become obnoxious and simply frustrating touch football puzzle family and talent shows are not how actual people behave it almost sickening another \u001b[31mbig\u001b[0m \u001b[31mflaw\u001b[0m is the woman carell is supposed to be falling for her in her first scene with steve carell is like watching stroke victim trying to be what imagine is supposed to be unique and original in this woman comes off \u001b[31mas\u001b[0m \u001b[31mmildly\u001b[0m retarded it makes me think that this movie \u001b[31mis\u001b[0m \u001b[31mtaking\u001b[0m place on another planet left the theater wondering what just \u001b[31msaw\u001b[0m after thinking further don \u001b[31mthink\u001b[0m \u001b[31mit\u001b[0m was much \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "the pace quick and \u001b[31menergetic\u001b[0m but most importantly he knows how to make comedy funny he doesn \u001b[31mthe\u001b[0m jokes and he understands that funny actors \u001b[31mknow\u001b[0m what they re doing and \u001b[31mhe\u001b[0m allows them \u001b[31mto\u001b[0m do it but segal goes step further he gives tommy boy friendly almost nostalgic tone that both the genuinely and the critics \u001b[31mdidn\u001b[0m like tommy boy shame on them movie doesn have to be super sophisticated or intellectual to be funny god farley and spade \u001b[31mwere\u001b[0m forced to do \u001b[31mmuted\u001b[0m comedy la the office this \u001b[31mis\u001b[0m great movie and one of \u001b[31mmy\u001b[0m all time favorites \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "for once story of hope over the tragic reality our youth face rising \u001b[31mdraws\u001b[0m \u001b[31mone\u001b[0m into scary and unfair world and shows through beautiful color \u001b[31mand\u001b[0m moving \u001b[31mmusic\u001b[0m how \u001b[31mone\u001b[0m man and his dedicated friends choose not to accept that world and change it through action and art an entertaining interesting emotional beautiful film showed this film to numerous high school students as well who all live in with poverty \u001b[31mand\u001b[0m and gun violence and they were with anderson the protagonist recommend this film to all ages over due to subtitles and some images of \u001b[31mdeath\u001b[0m \u001b[31mfrom\u001b[0m \u001b[31mall\u001b[0m backgrounds \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 0\n",
      "people and sleeping around that he kept secret from most people he feels free to have an affair with quasi because he kevin he figures out that he can fool some people with cards like hotel but it won get him out of those the of heaven are keeping track of him and everything he does after reading all the \u001b[31mtheories\u001b[0m on though it seems like identity is reminder of the different paths tony could ve taken in \u001b[31mhis\u001b[0m life possibly along with the \u001b[31mcar\u001b[0m joke \u001b[31minvolving\u001b[0m that made no sense to \u001b[31mme\u001b[0m \u001b[31motherwise\u001b[0m at that \u001b[31mpoint\u001b[0m \u001b[31mmy\u001b[0m \u001b[31mbrain\u001b[0m \u001b[31mout\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "over again can remember how many times he said the universe is made \u001b[31mout\u001b[0m of tiny little strings it like \u001b[31mthey\u001b[0m were \u001b[31mtrying\u001b[0m to us into just accepting are the best thing since bread finally the show ended off with an unpleasant sense of competition between and clearly biased towards this is supposed to be an educational program about quantum physics not about whether the us is better than \u001b[31meurope\u001b[0m or \u001b[31mvice\u001b[0m versa also felt that was part of the audiences need to see some conflict to remain interested please \u001b[31mgive\u001b[0m me \u001b[31mlittle\u001b[0m more credit than \u001b[31mthat\u001b[0m \u001b[31moverall\u001b[0m thumbs \u001b[31mdown\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "\u001b[31mthe\u001b[0m scenes involving joe character in particular the scenes in the terribly clich but still funny rich but screwed up \u001b[31mcharacters\u001b[0m house where the story towards it final moments can see how was great stage play and while the film makers did their best to translate this to celluloid it simply didn work and while laughed out loud at some of scenes and \u001b[31mone\u001b[0m liners think the first minutes my \u001b[31msenses\u001b[0m and expectations to such degree would have \u001b[31mlaughed\u001b[0m at \u001b[31manything\u001b[0m unless you re \u001b[31mstuck\u001b[0m for \u001b[31mnovelty\u001b[0m coffee coaster don pick this up \u001b[31mif\u001b[0m \u001b[31myou\u001b[0m see it in bargain bucket \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "for idx, data in enumerate(X_test[:10]):\n",
    "    print('y_true: {:d}'.format(y_test[idx]))\n",
    "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
    "    \n",
    "    # get the twenty most largest attention weights\n",
    "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        word_idx = data[_idx]\n",
    "        if word_idx != 0:\n",
    "            if _idx in large_weights_idx:\n",
    "                print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
    "            else:\n",
    "                print(tokenizer.index_word[word_idx], end=' ')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd09c8b33037c64074f7daf071dccd97f19741c28bd16c92540037a60c581174e3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
